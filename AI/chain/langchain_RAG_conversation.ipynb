{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith 추적을 시작합니다.\n",
      "[프로젝트명]\n",
      "langchain_RAG\n"
     ]
    }
   ],
   "source": [
    "from chain_logging import langsmith\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "langsmith(\"langchain_RAG\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG 템플릿 실험"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorstore not found. Creating a new one...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SSAFY\\miniconda3\\envs\\py310\\lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "c:\\Users\\SSAFY\\miniconda3\\envs\\py310\\lib\\site-packages\\langsmith\\client.py:354: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain import hub\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from vectorstore_manager import (\n",
    "    load_vectorstore,\n",
    "    create_vectorstore,\n",
    ")\n",
    "from config import PDF_PATH, VECTORSTORE_PATH, MODEL_PATH\n",
    "from utils import get_embeddings, format_docs_with_print\n",
    "\n",
    "\n",
    "# Load or create vectorstore\n",
    "if os.path.exists(VECTORSTORE_PATH):\n",
    "    print(\"Loading existing vectorstore...\")\n",
    "    embeddings = get_embeddings()\n",
    "    loaded_vectorstore = load_vectorstore(VECTORSTORE_PATH, embeddings)\n",
    "else:\n",
    "    print(\"Vectorstore not found. Creating a new one...\")\n",
    "    embeddings = get_embeddings()\n",
    "    loaded_vectorstore = create_vectorstore(PDF_PATH, VECTORSTORE_PATH, embeddings)\n",
    "\n",
    "\n",
    "# 단계 5: 리트리버 생성\n",
    "k = 2\n",
    "retriever = loaded_vectorstore.as_retriever(search_kwargs={\"k\": k})\n",
    "\n",
    "# 단계 6: 프롬프트 생성\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "# 단계7: 언어모델 생성\n",
    "# Ollama 모델을 불러옵니다.\n",
    "llm = ChatOllama(model=MODEL_PATH, temperature=0)\n",
    "\n",
    "\n",
    "# 단계 8: 체인 생성\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs_with_print, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HUMAN]\n",
      "유류비 청구는 어떻게 할 수 있나요? 제가 3박 4일로 부산에 갓다오는데 비용은 대충 어떻게 계산하죠?\n",
      "\n",
      "============================================================\n",
      "\n",
      "=== Retrieved Documents ===\n",
      "\n",
      "Document 1:\n",
      "..\\kisa_pdf\\3_06_출장규칙(240214).pdf |  page: 14\n",
      "그외지역:실비 70,000이내\n",
      " \n",
      "   ※ <삭제2017.12.29>\n",
      "   ※ <삭제2017.12.29.>\n",
      "   ※ 자가용승용차를이용하여출장가는경우의교통비는여행구간별철도운임또는버스운임\n",
      "(통상이용되는대중교통요금 )으로한다. 이때출장자는자가용승용차를이용하여출장을\n",
      "이행한사실을확인할수있는증거서류인고속도로통행영수증 (왕복), 출장일에출장지소재 \n",
      "주유소에서결제한신용카드매출전표 , 출장지주차영수증중1개이상의증거서류를갖추어\n",
      "제출해야한다.  다만, 부득이한사유로자가용승용차를이용한경우에는연료비및통행료를  \n",
      "지급할수있고이때의부득이한사유및연료비산정기준은공무원여비규정및인사혁신처  \n",
      "예규를적용하며그내용은아래와같다.<개정2017.12.29., 2021.12.28., 2022.5.27., 2023.3.13.>\n",
      "      업무형편상\n",
      "부득이한\n",
      "사유o산간오지,도서벽지등대중교통수단이 없어자가용을이용할수밖에\n",
      "없는경우\n",
      "o출장경로가매우복잡･\n",
      "다양하여대중교통을사실상이용할수없는경우\n",
      "o공무목적상부득이한심야시간대이동또는긴급한사유가있는경우\n",
      "--------------------------------------------------\n",
      "\n",
      "Document 2:\n",
      "..\\kisa_pdf\\3_06_출장규칙(240214).pdf |  page: 28\n",
      "국내 출장비 정산\n",
      "출장 변동사항 출장비 정산\n",
      "소 속 성 명 항 목 신청액 집행액 변동액 비 고\n",
      "교통비\n",
      "일  비\n",
      "식  비\n",
      "숙박비\n",
      "계\n",
      "교통비\n",
      "일  비\n",
      "식  비\n",
      "숙박비\n",
      "계\n",
      "교통비\n",
      "일  비\n",
      "식  비\n",
      "숙박비\n",
      "계\n",
      "교통비\n",
      "일  비\n",
      "식  비\n",
      "숙박비\n",
      "계\n",
      "합 계\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Anser: \n",
      "\n",
      "유류비 청구는 자가용승용차를 이용하여 출장가는 경우의 교통비로, 여행 구간별 철도운임 또는 버스운임 (통상 이용되는 대중교통 요금) 으로 한다. 출장자는 고속도로 통행 영수증, 출장 일에 출장지 소재 주유소에서 결제한 신용 카드 매출 전표, 출장 지 주차 영수증 중 1 개 이상의 증거 서류를 갖추어 제출해야 한다."
     ]
    }
   ],
   "source": [
    "from chain_output_stream import stream_response\n",
    "\n",
    "# 단계 8: 체인 실행(Run Chain)\n",
    "# 문서에 대한 질의를 입력하고, 답변을 출력합니다.\n",
    "# question = \"구글 딥마인드 정책에 대해서 알려주세요\"\n",
    "# question = \"미래의 AI 소프트웨어 매출 전망은 어떻게 되나요?\"\n",
    "# question = \"YouTube 가 2024년에 의무화 한 것은 무엇인가요?\"\n",
    "# question = \"연차는 어떻게 사용할 수 있나요?\"\n",
    "question = \"유류비 청구는 어떻게 할 수 있나요? 제가 3박 4일로 부산에 갓다오는데 비용은 대충 어떻게 계산하죠?\"\n",
    "\n",
    "response = rag_chain.stream(question)\n",
    "\n",
    "# 결과 출력\n",
    "print(f\"[HUMAN]\\n{question}\\n\")\n",
    "print(\"===\" * 20)\n",
    "# print(rag_chain[\"retrieved_docs\"])\n",
    "answer = stream_response(response, return_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 대화 내용을 기억하는 기능을 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing vectorstore...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SSAFY\\miniconda3\\envs\\py39_cuda\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "c:\\Users\\SSAFY\\ssafy\\자율프로젝트\\S11P31S102\\AI\\chain\\vectorstore_manager.py:42: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferWindowMemory(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing Local Model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5e717ac15c344959a9b3f5928a62d0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from vectorstore_manager import (\n",
    "    load_vectorstore,\n",
    "    create_vectorstore,\n",
    "    create_conversaion_memory,\n",
    ")\n",
    "from config import PDF_PATH, VECTORSTORE_PATH, MODEL_PATH, LOCAL_MODEL_PATH, MODEL_NAME\n",
    "from utils import get_embeddings, format_docs_with_print, create_prompt\n",
    "from load_model import load_model, create_model\n",
    "\n",
    "# Load or create vectorstore\n",
    "if os.path.exists(VECTORSTORE_PATH):\n",
    "    print(\"Loading existing vectorstore...\")\n",
    "    embeddings = get_embeddings()\n",
    "    loaded_vectorstore = load_vectorstore(VECTORSTORE_PATH, embeddings)\n",
    "else:\n",
    "    print(\"Vectorstore not found. Creating a new one...\")\n",
    "    embeddings = get_embeddings()\n",
    "    loaded_vectorstore = create_vectorstore(PDF_PATH, VECTORSTORE_PATH, embeddings)\n",
    "\n",
    "# 대화를 기억할 메모리 객체 생성\n",
    "memory = create_conversaion_memory(\"chat_history\", \"answer\")\n",
    "\n",
    "# 단계 5: 리트리버 생성\n",
    "k = 2\n",
    "retriever = loaded_vectorstore.as_retriever(search_kwargs={\"k\": k})\n",
    "\n",
    "# 단계 6: 프롬프트 생성\n",
    "prompt = create_prompt()\n",
    "\n",
    "# 단계7: 언어모델 생성\n",
    "# Ollama 모델을 불러옵니다.\n",
    "# llm = ChatOllama(model=MODEL_PATH, temperature=0)\n",
    "\n",
    "if os.path.exists(LOCAL_MODEL_PATH):\n",
    "    print(\"Loading existing Local Model...\")\n",
    "    llm = load_model(LOCAL_MODEL_PATH)\n",
    "else:\n",
    "    print(\"Local Model not found. Creating a new one...\")\n",
    "    create_model(MODEL_NAME,LOCAL_MODEL_PATH)\n",
    "    llm = load_model(LOCAL_MODEL_PATH)\n",
    "\n",
    "\n",
    "# 단계 8: 체인 생성\n",
    "# 단계 8: 체인 생성\n",
    "rag_chain = (\n",
    "    {\n",
    "        \"context\": retriever | format_docs_with_print,\n",
    "        \"question\": RunnablePassthrough(),\n",
    "        \"chat_history\": lambda x: memory.load_memory_variables({})[\"chat_history\"],\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HUMAN]\n",
      "연차는 어떻게 사용할 수 있나요?\n",
      "\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SSAFY\\miniconda3\\envs\\py39_cuda\\lib\\site-packages\\transformers\\generation\\utils.py:1885: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n",
      "Exception in thread Thread-8:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\SSAFY\\miniconda3\\envs\\py39_cuda\\lib\\threading.py\", line 980, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"c:\\Users\\SSAFY\\miniconda3\\envs\\py39_cuda\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"c:\\Users\\SSAFY\\miniconda3\\envs\\py39_cuda\\lib\\threading.py\", line 917, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"c:\\Users\\SSAFY\\miniconda3\\envs\\py39_cuda\\lib\\site-packages\\torch\\utils\\_contextlib.py\", line 116, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"c:\\Users\\SSAFY\\miniconda3\\envs\\py39_cuda\\lib\\site-packages\\transformers\\generation\\utils.py\", line 2024, in generate\n",
      "    result = self._sample(\n",
      "  File \"c:\\Users\\SSAFY\\miniconda3\\envs\\py39_cuda\\lib\\site-packages\\transformers\\generation\\utils.py\", line 2982, in _sample\n",
      "    outputs = self(**model_inputs, return_dict=True)\n",
      "  File \"c:\\Users\\SSAFY\\miniconda3\\envs\\py39_cuda\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"c:\\Users\\SSAFY\\miniconda3\\envs\\py39_cuda\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"c:\\Users\\SSAFY\\miniconda3\\envs\\py39_cuda\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py\", line 1189, in forward\n",
      "    outputs = self.model(\n",
      "  File \"c:\\Users\\SSAFY\\miniconda3\\envs\\py39_cuda\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"c:\\Users\\SSAFY\\miniconda3\\envs\\py39_cuda\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"c:\\Users\\SSAFY\\miniconda3\\envs\\py39_cuda\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py\", line 950, in forward\n",
      "    inputs_embeds = self.embed_tokens(input_ids)\n",
      "  File \"c:\\Users\\SSAFY\\miniconda3\\envs\\py39_cuda\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"c:\\Users\\SSAFY\\miniconda3\\envs\\py39_cuda\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"c:\\Users\\SSAFY\\miniconda3\\envs\\py39_cuda\\lib\\site-packages\\torch\\nn\\modules\\sparse.py\", line 190, in forward\n",
      "    return F.embedding(\n",
      "  File \"c:\\Users\\SSAFY\\miniconda3\\envs\\py39_cuda\\lib\\site-packages\\torch\\nn\\functional.py\", line 2551, in embedding\n",
      "    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)\n",
      "RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Retrieved Documents ===\n",
      "\n",
      "Document 1:\n",
      "..\\kisa_pdf\\3_07_취업규칙(240214).pdf |  page: 6\n",
      "1. 1주40시간근무자가매년말일기준으로잔여보상휴가가 200시간(25x8시간)을초과하는경우\n",
      "(단시간근로자는 1주40시간근무자에비례하여계산한다 .)\n",
      "  2. 계속근로연수가 1년미만의기간에발생하는최대11일의연차유급휴가인경우\n",
      "  ② 제1항에따라이월·저축된저축연차유급휴가에 대해서는미사용연차유급휴가수당을 지급하\n",
      "지아니한다 .<신설2021.12.28.>\n",
      "  ③ 저축한연차유급휴가는 10일이상의장기휴가로사용하거나분할하여  사용할수있다. 단, 10일\n",
      "이상의장기휴가는 3개월이전에신청하여야한다.<신설2021.12.28.>\n",
      "제25조의3(연차대출제 )  직원은해당연도의연차유급휴가일수 및잔여보상휴가를모두사용한때에\n",
      "는다음연도의연차유급휴가중의일부를다음각호의기준에따라미리사용할수있다.<신설\n",
      "2021.12.28.> <개정2022. 5. 27.>\n",
      "  1. 1년미만재직: 최대5일\n",
      "  2. 1년이상2년미만재직: 최대6일\n",
      "  3. 2년이상3년미만재직: 최대7일\n",
      "  4. 3년이상4년미만재직: 최대8일\n",
      "  5. 4년이상재직: 최대10일\n",
      "--------------------------------------------------\n",
      "\n",
      "Document 2:\n",
      "..\\kisa_pdf\\3_08_계약직원 관리규칙(201229).pdf |  page: 15\n",
      "1인에게만지급\n",
      "․급여규정및급여\n",
      "지급규칙에따라\n",
      "지급\n",
      "시간외및\n",
      "휴일근무\n",
      "수당정상근무시간외\n",
      "초과근무를한자o통상임금×1.5÷209×\n",
      "시간외및휴일근무시간수․급여규정및급여\n",
      "지급규칙에따라\n",
      "지급\n",
      "연차수당연차휴가를\n",
      "사용하지\n",
      "않은자o통상임금×1÷209×8×\n",
      "미사용연차휴가일수\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Anser: \n",
      "\n"
     ]
    },
    {
     "ename": "Empty",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mEmpty\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[HUMAN]\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mquestion\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m===\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m20\u001b[39m)\n\u001b[1;32m---> 14\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[43mstream_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# 메모리에 대화 내용을 저장\u001b[39;00m\n\u001b[0;32m     17\u001b[0m memory\u001b[38;5;241m.\u001b[39msave_context({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m: question}, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m\"\u001b[39m: answer})\n",
      "File \u001b[1;32mc:\\Users\\SSAFY\\ssafy\\자율프로젝트\\S11P31S102\\AI\\chain\\chain_output_stream.py:25\u001b[0m, in \u001b[0;36mstream_response\u001b[1;34m(response, return_output)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;124;03mAI 모델로부터의 응답을 스트리밍하여 각 청크를 처리하면서 출력합니다.\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;124;03m- str: `return_output`이 True인 경우, 연결된 응답 문자열입니다. 그렇지 않으면, 아무것도 반환되지 않습니다.\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     24\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m response:\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(token, AIMessageChunk):\n\u001b[0;32m     27\u001b[0m         answer \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m token\u001b[38;5;241m.\u001b[39mcontent\n",
      "File \u001b[1;32mc:\\Users\\SSAFY\\miniconda3\\envs\\py39_cuda\\lib\\site-packages\\langchain_core\\runnables\\base.py:3407\u001b[0m, in \u001b[0;36mRunnableSequence.stream\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   3401\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstream\u001b[39m(\n\u001b[0;32m   3402\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   3403\u001b[0m     \u001b[38;5;28minput\u001b[39m: Input,\n\u001b[0;32m   3404\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   3405\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[0;32m   3406\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[Output]:\n\u001b[1;32m-> 3407\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(\u001b[38;5;28miter\u001b[39m([\u001b[38;5;28minput\u001b[39m]), config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\SSAFY\\miniconda3\\envs\\py39_cuda\\lib\\site-packages\\langchain_core\\runnables\\base.py:3394\u001b[0m, in \u001b[0;36mRunnableSequence.transform\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   3388\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtransform\u001b[39m(\n\u001b[0;32m   3389\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   3390\u001b[0m     \u001b[38;5;28minput\u001b[39m: Iterator[Input],\n\u001b[0;32m   3391\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   3392\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[0;32m   3393\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[Output]:\n\u001b[1;32m-> 3394\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform_stream_with_config(\n\u001b[0;32m   3395\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m   3396\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform,\n\u001b[0;32m   3397\u001b[0m         patch_config(config, run_name\u001b[38;5;241m=\u001b[39m(config \u001b[38;5;129;01mor\u001b[39;00m {})\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m   3398\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   3399\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\SSAFY\\miniconda3\\envs\\py39_cuda\\lib\\site-packages\\langchain_core\\runnables\\base.py:2197\u001b[0m, in \u001b[0;36mRunnable._transform_stream_with_config\u001b[1;34m(self, input, transformer, config, run_type, **kwargs)\u001b[0m\n\u001b[0;32m   2195\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   2196\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m-> 2197\u001b[0m         chunk: Output \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m   2198\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m chunk\n\u001b[0;32m   2199\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m final_output_supported:\n",
      "File \u001b[1;32mc:\\Users\\SSAFY\\miniconda3\\envs\\py39_cuda\\lib\\site-packages\\langchain_core\\runnables\\base.py:3357\u001b[0m, in \u001b[0;36mRunnableSequence._transform\u001b[1;34m(self, input, run_manager, config, **kwargs)\u001b[0m\n\u001b[0;32m   3354\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3355\u001b[0m         final_pipeline \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39mtransform(final_pipeline, config)\n\u001b[1;32m-> 3357\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m final_pipeline\n",
      "File \u001b[1;32mc:\\Users\\SSAFY\\miniconda3\\envs\\py39_cuda\\lib\\site-packages\\langchain_core\\output_parsers\\transform.py:64\u001b[0m, in \u001b[0;36mBaseTransformOutputParser.transform\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtransform\u001b[39m(\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28minput\u001b[39m: Iterator[Union[\u001b[38;5;28mstr\u001b[39m, BaseMessage]],\n\u001b[0;32m     51\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m     53\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[T]:\n\u001b[0;32m     54\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Transform the input into the output format.\u001b[39;00m\n\u001b[0;32m     55\u001b[0m \n\u001b[0;32m     56\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;124;03m        The transformed output.\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 64\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform_stream_with_config(\n\u001b[0;32m     65\u001b[0m         \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform, config, run_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparser\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     66\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\SSAFY\\miniconda3\\envs\\py39_cuda\\lib\\site-packages\\langchain_core\\runnables\\base.py:2161\u001b[0m, in \u001b[0;36mRunnable._transform_stream_with_config\u001b[1;34m(self, input, transformer, config, run_type, **kwargs)\u001b[0m\n\u001b[0;32m   2159\u001b[0m input_for_tracing, input_for_transform \u001b[38;5;241m=\u001b[39m tee(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m   2160\u001b[0m \u001b[38;5;66;03m# Start the input iterator to ensure the input Runnable starts before this one\u001b[39;00m\n\u001b[1;32m-> 2161\u001b[0m final_input: Optional[Input] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minput_for_tracing\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m   2162\u001b[0m final_input_supported \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   2163\u001b[0m final_output: Optional[Output] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\SSAFY\\miniconda3\\envs\\py39_cuda\\lib\\site-packages\\langchain_core\\runnables\\base.py:1431\u001b[0m, in \u001b[0;36mRunnable.transform\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   1428\u001b[0m             final \u001b[38;5;241m=\u001b[39m ichunk\n\u001b[0;32m   1430\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m got_first_val:\n\u001b[1;32m-> 1431\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream(final, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\SSAFY\\miniconda3\\envs\\py39_cuda\\lib\\site-packages\\langchain_core\\language_models\\llms.py:576\u001b[0m, in \u001b[0;36mBaseLLM.stream\u001b[1;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[0;32m    569\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    570\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(\n\u001b[0;32m    571\u001b[0m         e,\n\u001b[0;32m    572\u001b[0m         response\u001b[38;5;241m=\u001b[39mLLMResult(\n\u001b[0;32m    573\u001b[0m             generations\u001b[38;5;241m=\u001b[39m[[generation]] \u001b[38;5;28;01mif\u001b[39;00m generation \u001b[38;5;28;01melse\u001b[39;00m []\n\u001b[0;32m    574\u001b[0m         ),\n\u001b[0;32m    575\u001b[0m     )\n\u001b[1;32m--> 576\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    577\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    578\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_llm_end(LLMResult(generations\u001b[38;5;241m=\u001b[39m[[generation]]))\n",
      "File \u001b[1;32mc:\\Users\\SSAFY\\miniconda3\\envs\\py39_cuda\\lib\\site-packages\\langchain_core\\language_models\\llms.py:560\u001b[0m, in \u001b[0;36mBaseLLM.stream\u001b[1;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[0;32m    558\u001b[0m generation: Optional[GenerationChunk] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    559\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 560\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stream(\n\u001b[0;32m    561\u001b[0m         prompt, stop\u001b[38;5;241m=\u001b[39mstop, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    562\u001b[0m     ):\n\u001b[0;32m    563\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m chunk\u001b[38;5;241m.\u001b[39mtext\n\u001b[0;32m    564\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m generation \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\SSAFY\\miniconda3\\envs\\py39_cuda\\lib\\site-packages\\langchain_huggingface\\llms\\huggingface_pipeline.py:355\u001b[0m, in \u001b[0;36mHuggingFacePipeline._stream\u001b[1;34m(self, prompt, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    352\u001b[0m t1 \u001b[38;5;241m=\u001b[39m Thread(target\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpipeline\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mgenerate, kwargs\u001b[38;5;241m=\u001b[39mgeneration_kwargs)\n\u001b[0;32m    353\u001b[0m t1\u001b[38;5;241m.\u001b[39mstart()\n\u001b[1;32m--> 355\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m char \u001b[38;5;129;01min\u001b[39;00m streamer:\n\u001b[0;32m    356\u001b[0m     chunk \u001b[38;5;241m=\u001b[39m GenerationChunk(text\u001b[38;5;241m=\u001b[39mchar)\n\u001b[0;32m    357\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m run_manager:\n",
      "File \u001b[1;32mc:\\Users\\SSAFY\\miniconda3\\envs\\py39_cuda\\lib\\site-packages\\transformers\\generation\\streamers.py:223\u001b[0m, in \u001b[0;36mTextIteratorStreamer.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 223\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    224\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_signal:\n\u001b[0;32m    225\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m()\n",
      "File \u001b[1;32mc:\\Users\\SSAFY\\miniconda3\\envs\\py39_cuda\\lib\\queue.py:179\u001b[0m, in \u001b[0;36mQueue.get\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    177\u001b[0m         remaining \u001b[38;5;241m=\u001b[39m endtime \u001b[38;5;241m-\u001b[39m time()\n\u001b[0;32m    178\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m remaining \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[1;32m--> 179\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[0;32m    180\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnot_empty\u001b[38;5;241m.\u001b[39mwait(remaining)\n\u001b[0;32m    181\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get()\n",
      "\u001b[1;31mEmpty\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from chain_output_stream import stream_response\n",
    "\n",
    "# 단계 8: 체인 실행(Run Chain)\n",
    "# 문서에 대한 질의를 입력하고, 답변을 출력합니다.\n",
    "question = \"연차는 어떻게 사용할 수 있나요?\"\n",
    "# question = \"유류비 청구는 어떻게 할 수 있나요? 제가 3박 4일로 부산에 갓다오는데 비용은 대충 어떻게 계산하죠?\"\n",
    "# question = \"아까 물어본 질문을 다시 알려주세요\"\n",
    "\n",
    "response = rag_chain.stream(question)\n",
    "\n",
    "# 결과 출력\n",
    "print(f\"[HUMAN]\\n{question}\\n\")\n",
    "print(\"===\" * 20)\n",
    "answer = stream_response(response, return_output=True)\n",
    "\n",
    "# 메모리에 대화 내용을 저장\n",
    "memory.save_context({\"question\": question}, {\"answer\": answer})\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
