{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith 추적을 시작합니다.\n",
      "[프로젝트명]\n",
      "langchain_RAG\n"
     ]
    }
   ],
   "source": [
    "from chain_logging import langsmith\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "langsmith(\"langchain_RAG\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 대화 내용을 기억하는 기능을 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing vectorstore...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SSAFY\\ssafy\\자율프로젝트\\S11P31S102\\AI\\chain\\vectorstore_manager.py:42: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferWindowMemory(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from vectorstore_manager import (\n",
    "    load_vectorstore,\n",
    "    create_vectorstore,\n",
    "    create_conversaion_memory,\n",
    ")\n",
    "from config import PDF_PATH, VECTORSTORE_PATH, MODEL_PATH, LOCAL_MODEL_PATH, MODEL_NAME\n",
    "from utils import get_embeddings, format_docs_with_print, create_prompt\n",
    "from load_model import load_model, create_model\n",
    "\n",
    "# Load or create vectorstore\n",
    "if os.path.exists(VECTORSTORE_PATH):\n",
    "    print(\"Loading existing vectorstore...\")\n",
    "    embeddings = get_embeddings()\n",
    "    loaded_vectorstore = load_vectorstore(VECTORSTORE_PATH, embeddings)\n",
    "else:\n",
    "    print(\"Vectorstore not found. Creating a new one...\")\n",
    "    embeddings = get_embeddings()\n",
    "    loaded_vectorstore = create_vectorstore(PDF_PATH, VECTORSTORE_PATH, embeddings)\n",
    "\n",
    "# 대화를 기억할 메모리 객체 생성\n",
    "memory = create_conversaion_memory(\"chat_history\", \"answer\")\n",
    "\n",
    "# 단계 5: 리트리버 생성\n",
    "k = 3\n",
    "retriever = loaded_vectorstore.as_retriever(search_kwargs={\"k\": k})\n",
    "\n",
    "# 단계 6: 프롬프트 생성\n",
    "prompt = create_prompt()\n",
    "\n",
    "# 단계7: 언어모델 생성\n",
    "# Ollama 모델을 불러옵니다.\n",
    "# llm = ChatOllama(model=MODEL_PATH, temperature=0)\n",
    "\n",
    "# 객체 생성\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0.1,  # 창의성 (0.0 ~ 2.0)\n",
    "    model_name=MODEL_PATH,  # 모델명\n",
    ")\n",
    "\n",
    "# if os.path.exists(LOCAL_MODEL_PATH):\n",
    "#     print(\"Loading existing Local Model...\")\n",
    "#     llm = load_model(LOCAL_MODEL_PATH)\n",
    "# else:\n",
    "#     print(\"Local Model not found. Creating a new one...\")\n",
    "#     create_model(MODEL_NAME,LOCAL_MODEL_PATH)\n",
    "#     llm = load_model(LOCAL_MODEL_PATH)\n",
    "\n",
    "\n",
    "# 단계 8: 체인 생성\n",
    "# 단계 8: 체인 생성\n",
    "rag_chain = (\n",
    "    {\n",
    "        \"context\": retriever | format_docs_with_print,\n",
    "        \"question\": RunnablePassthrough(),\n",
    "        \"chat_history\": lambda x: memory.load_memory_variables({})[\"chat_history\"],\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HUMAN]\n",
      "언제 퇴직금이 지급되나요?\n",
      "\n",
      "============================================================\n",
      "\n",
      "=== Retrieved Documents ===\n",
      "\n",
      "Document 1:\n",
      "..\\kisa_pdf\\3_09_급여지급규칙(240214).pdf |  page: 9\n",
      "[별지제4호]<개정2009. 12. 24>퇴직금 중간정산 신청서결재담  당 접수담 당\n",
      "신청일자신청번호부서(팀) 성명(사번)직   급 입사일자주   소정산기간   년 ~    년 위상기명본인은급여규정제17조및급여지급규칙제13조에따라퇴직금중간정산을신청합니다.\n",
      "--------------------------------------------------\n",
      "\n",
      "Document 2:\n",
      "..\\kisa_pdf\\3_07_취업규칙(240214).pdf |  page: 4\n",
      "1. 15일의연차유급휴가  2. 입사후1년미만의기간에대해발생하는연차유급휴가(단, 입사당해연도에발생한연차유급휴가일수는제외)  ⑦ 퇴직시에는 입사일을기준으로연차유급휴가를 정산한다.<신설2021.12.28.>제22조(공가) 직원이다음각호의어느하나에해당하는경우에는이에필요한기간동안공가를부여한다.  1. 병역법기타다른법령에의한징병검사･동원또는훈련에참가할때  2. 국회･법원･검찰기타국가기관의공무로인하여소환된때  3. 법률의규정에의하여투표에참가할때  4. 천재지변, 교통차단등기타의사유로출근이불가능할때  5. 기타원장이필요하다고인정할때  6. 「산업안전보건법」 제43조에따른건강진단또는「국민건강보험법」 제52조에따른건강검진을받을때<신설2019. 3.28>제23조(병가) ①직원이다음각호의어느하나에해당하는경우에는연60일의범위내에서병가를부여할수있다. 단, 공무상질병또는상해로인한경우에는그기간을연180일범위내에서부여할수있다. <개정2014. 8. 28., 2022. 5. 27.>  1. 질병또는부상으로인하여직무를수행할수없을때\n",
      "--------------------------------------------------\n",
      "\n",
      "Document 3:\n",
      "..\\kisa_pdf\\2_06_급여규정(240221).pdf |  page: 1\n",
      "여는신규채용,승진,정직,감봉또는복직등의인사발령일을기준으로일할계산한다.③5년이상근속한직원이퇴직,사망또는휴직시는그날이속하는월에15일이상근무시연봉의12분의1에해당하는금액전액을지급하며,월15일미만인경우에는일할계산하여지급한다.<개정2014.9.5.>④급여계산에있어서원이하의단위는절사한다.제8조(휴직자의급여)다음각호의어느하나에해당하는사유로휴직한자를제외하고는휴직기간중급여를지급하지아니한다.\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Anser: \n",
      "\n",
      "퇴직금은 퇴직 시 정산되며, 입사일을 기준으로 연차유급휴가를 정산한 후 지급됩니다. 구체적인 지급일은 회사의 규정에 따라 다를 수 있습니다."
     ]
    }
   ],
   "source": [
    "from chain_output_stream import stream_response\n",
    "\n",
    "# 단계 8: 체인 실행(Run Chain)\n",
    "# 문서에 대한 질의를 입력하고, 답변을 출력합니다.\n",
    "# question = \"연차는 어떻게 사용할 수 있나요?\"\n",
    "# question = \"유류비 청구는 어떻게 할 수 있나요? 제가 3박 4일로 부산에 갔다오는데 비용은 대충 어떻게 계산하죠?\"\n",
    "# question = \"아까 물어본 질문을 다시 알려주세요\"\n",
    "# question = \"급여 지급일은 언제인가요?\"\n",
    "question = \"언제 퇴직금이 지급되나요?\"\n",
    "\n",
    "response = rag_chain.stream(question)\n",
    "\n",
    "# 결과 출력\n",
    "print(f\"[HUMAN]\\n{question}\\n\")\n",
    "print(\"===\" * 20)\n",
    "answer = stream_response(response, return_output=True)\n",
    "\n",
    "# 메모리에 대화 내용을 저장\n",
    "memory.save_context({\"question\": question}, {\"answer\": answer})\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310_cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
